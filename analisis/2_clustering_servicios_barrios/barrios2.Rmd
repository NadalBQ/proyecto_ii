---
title: "Análisis de similitud de barrios por servicios"
author: "María"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


El segundo objetivo consiste en hacer un análisis cluster para agrupar las viviendas por barrios, pero teniendo en cuenta variables del entorno que podrían influir a la hora de alquilar una vivienda en una ciudad (por ejemplo el transporte o la cercanía a algunos lugares).


# Carga y preparación de datos

Se importan a continuación todas las librerías utilizadas.

```{r message=FALSE, warning=FALSE}
library(knitr)
library(sf)  
library(dplyr)  
library(geosphere) 
library(leaflet) # Crear mapas interactivos (visualización geoespacial).
library(tidyr) 
library(ggplot2) 
library(car)     # Regresión lineal (VIF)
library(grid)
library(gridExtra)
library(pls)  
library(ggsci)  
library(cluster) 
library(FactoMineR) 
library(factoextra) 
library(NbClust) # Determina el número óptimo de clusters usando múltiples índices
library(clValid) # Valida la calidad de los clusters generados.
```

Para continuar con el proyecto, se han obtenido nuevas bases de datos para añadir información sobre los barrios y poder agruparlos en función de cómo afectan estas nuevas variables al precio ponderado obtenido anteriormente.

Se han obtenido 3 nuevos datasets:

- Transporte público por barrio, obtenido de la página del ayuntamiento.

- Renta media, obtenido en la página del INE.

- Distancias a lugares de interés, que se ha obtenido calculando las distancias a partir del fichero geojson de valencia y las coordenadas de cada lugar.

Primero se procederá a leer los archivos de pisos del estudio anterior y el fichero de transporte:

```{r pisos y transporte}
pisos = read.csv('pisos_limpios.csv', sep =',')  # Resultado del objetivo 1
transporte = read.csv('transporte.csv', sep = ';') # Sacado de la pagina del ayuntamiento
```

A continuación, se van a calcular los centroides de cada barrio a partir del geojson para así poder calcular la distancia exacta del centro del barrio al lugar de interés:

```{r centroides barrios}

# Cargar el geojson
barrios <- st_read("valencia.geojson")

# Obtener centroides de los polígonos (barrios) y extraer sus coordenadas
barrios_centroides <- barrios %>%  
  mutate(centroide = st_centroid(geometry)) %>%
  mutate(lon = st_coordinates(centroide)[,1], # Extrae longitud
         lat = st_coordinates(centroide)[,2]) # Extrae latitud

```
Ahora se crea un nuevo fichero con los puntos clave que se han elegido para el estudio:

```{r lugares de interés}

puntos_clave <- data.frame(
  nombre = c("Plaza_Ayuntamiento", "CAC", "Bioparc", "Estacion_Norte", 
             "Playa_Malvarrosa", "Torres_Serranos", "Universitat", 
             "Puerto", "Albufera", "Viveros", 'UPV', "Mestalla"),
  lat = c(39.4699, 39.4554, 39.4781, 39.4652, 
          39.4831, 39.4793, 39.4754, 
          39.4515, 39.3331, 39.4818, 39.4823, 39.4748),
  lon = c(-0.3763, -0.3537, -0.4008, -0.3767, 
          -0.3282, -0.3758, -0.3445, 
          -0.3160, -0.3356, -0.3677, -0.3435, -0.3574)
)
```

Para comprobar que las coordenadas son correctas se muestran en un mapa:

```{r mapa lugares}

# Convierte el data frame a un objeto espacial (sf), usando lon/lat como coordenadas
lugares_sf <- st_as_sf(puntos_clave, coords = c("lon", "lat"), crs = 4326)

leaflet() %>%
  addProviderTiles("CartoDB.Positron") %>% 
  addPolygons(data = barrios, 
              color = "#444444",
              weight = 1,        
              fillOpacity = 0.3, 
              label = ~as.character(nombre),  # nombre barrios
              highlightOptions = highlightOptions(weight = 2, color = "blue", fillOpacity = 0.5)) %>% 
  addCircleMarkers(data = lugares_sf, 
                   radius = 6, 
                   color = "red", 
                   fillOpacity = 0.9,
                   label = ~nombre,
                   popup = ~nombre)

```

Efectivamente, cada lugar se muestra donde corresponde, por lo que se puede continuar con el estudio.
  
Finalmente, se calculan las distancias en kilómetros entre cada barrio y cada punto de interés mediante el uso de la función distHaversine. Se crea el nuevo dataset de distancias:

```{r distancias}

# Función para calcular distancia entre dos coordenadas
calcular_distancias <- function(lat_barrio, lon_barrio, puntos) {
  distHaversine(matrix(c(lon_barrio, lat_barrio), ncol = 2), 
                matrix(c(puntos$lon, puntos$lat), ncol = 2)) / 1000  # en km
}

distancias <- barrios_centroides %>%
  rowwise() %>%  
  mutate(across(c(lat, lon), as.numeric)) %>% 
  mutate(
    distancia_P_Ayuntamiento = calcular_distancias(lat, lon, puntos_clave[1,]),
    distancia_CAC           = calcular_distancias(lat, lon, puntos_clave[2,]),
    distancia_Bioparc       = calcular_distancias(lat, lon, puntos_clave[3,]),
    distancia_Estacion      = calcular_distancias(lat, lon, puntos_clave[4,]),
    distancia_Malvarrosa    = calcular_distancias(lat, lon, puntos_clave[5,]),
    distancia_Torres        = calcular_distancias(lat, lon, puntos_clave[6,]),
    distancia_UV   = calcular_distancias(lat, lon, puntos_clave[7,]),
    distancia_Puerto        = calcular_distancias(lat, lon, puntos_clave[8,]),
    distancia_Albufera      = calcular_distancias(lat, lon, puntos_clave[9,]),
    distancia_Viveros      = calcular_distancias(lat, lon, puntos_clave[10,]),
    distancia_UPV      = calcular_distancias(lat, lon, puntos_clave[11,]),
    distancia_Mestalla = calcular_distancias(lat, lon, puntos_clave[12,])
  ) %>%
  ungroup()
```

Como en el fichero hay variables no deseadas del geojson, se filtra para que sea más cómodo de usar:

```{r filtrar distancias}
distancias_filtrado = distancias %>% select(nombre, distancia_P_Ayuntamiento:last_col())
```

```{r distancias final}
# Se elimina la columna de geometría espacial 
distancias_f <- st_drop_geometry(distancias_filtrado)
```

Una vez obtenidas las distancias, se carga el fichero de renta media:

```{r renta}
renta_media = read.csv("ineValenciafinal.csv", sep =";")
head(renta_media)
```

# Dataset barrios

En este apartado se procede a la creación del dataset final de los barrios. 
Primero se va a juntar la información del dataset de los pisos con el de las distancias. Para ello, como se va a concatenar ambos conjuntos de datos por el nombre del barrio, se comprueba si algún barrio presente en pisos no se encuentra en distancias:

```{r dif pisos y distancias}
setdiff(unique(pisos$neighbourhood_cleansed), unique(distancias_f$nombre))
```

Se muestra el barrio Mont-Olivet, vamos a ver si realmente no se encuentra en distancias:

```{r barrios distancias}
unique(distancias_f$nombre)
```

Se puede observar que si se encuentra en el fichero, solo que en el de pisos es MONT-OLIVET y en el de distancias es MONTOLIVET, por lo que se le añade el guión:

```{r renombrar}
distancias_f$nombre[distancias_f$nombre == "MONTOLIVET"] <- "MONT-OLIVET"
```

Ahora sí se pueden juntar ambos ficheros. 

Antes, se crea el fichero barrios_info con la variable del nombre del barrio y la media de la variable precio_ponderado por cada barrio. Como la variable precio_ponderado daba problemas al estar indicada como carácter, se convierte en numérica para así poder calcular la media.

```{r precio ponderado barrios,  warning=FALSE}
pisos$precio_ponderado <- gsub(",", ".", pisos$precio_ponderado) # cambiar coma decimal por punto
pisos$precio_ponderado <- as.numeric(pisos$precio_ponderado) # convertir a numérico

barrios_info <- pisos %>%
  group_by(neighbourhood_group_cleansed, neighbourhood_cleansed) %>%
  summarise(
    media = mean(precio_ponderado, na.rm = TRUE) # Calcula el precio promedio (media) ponderado en cada grupo/barrio, ignorando NAs
  ) %>%
  arrange(desc(media))  # Ordena los resultados de mayor a menor precio promedio
```

A continuación, se añaden las distancias:

```{r join barriosxdistancias}
barrios_info <- barrios_info %>%
  left_join(distancias_f, by = c("neighbourhood_cleansed" = "nombre"))
```

Se realiza el mismo procedimiento con el fichero transporte, comprobando si algún barrio no está presente en transportes:

```{r dif barrios transporte}
setdiff(unique(barrios_info$neighbourhood_cleansed), unique(transporte$nombre))
```

Como todos los barrios están presentes, se unen ambos ficheros con las variables que se desean usar de transporte y se eliminan los ausentes:

```{r transporte resumen}
transporte_clean <- transporte %>%
  filter(!is.na(transporte) & transporte != "")  # elimina filas sin tipo transporte

# Agrupa y cuenta 
transporte_resumen <- transporte_clean %>%
  group_by(nombre, codbarrio, coddistrit, transporte) %>%
  summarise(conteo = n(), .groups = "drop") %>%
  pivot_wider(
    names_from = transporte,
    values_from = conteo,
    values_fill = 0
  )
```

```{r join barriosxtransporte}
barrios_info <- barrios_info %>%
  left_join(transporte_resumen, by = c("neighbourhood_cleansed" = "nombre"))
```

Por último, se añade el fichero renta_media. Como los nombres de los barrios no coinciden con los de barrios_info, no se puede unir por nombre. En su lugar, se usan los códigos codbarrio y coddistrit, también presentes en el fichero de transporte. Se comprueba si todos los pares (distrito, barrio) de renta_media están en barrios_info antes de unirlos.

```{r pares únicos,  warning=FALSE}
# Pares únicos en barrios_info
pares_barrios_info <- barrios_info %>%
  select(coddistrit, codbarrio) %>%
  distinct()

# Pares únicos en renta_media 
pares_renta_media <- renta_media %>%
  select(Codigo_Distrito, Codigo_Barrio) %>%
  distinct()
```

```{r diferencias}
diferencias <- anti_join(pares_barrios_info, pares_renta_media,
                         by = c("coddistrit" = "Codigo_Distrito", 
                                "codbarrio" = "Codigo_Barrio"))
head(diferencias)
```

Como no hay ninguna diferencia, se pueden juntar sin problema ambas bases de datos con las variables de renta_media que se desean:

```{r join barriosxrenta,  warning=FALSE}
renta_media_seleccion <- renta_media %>%
  group_by(Codigo_Distrito, Codigo_Barrio) %>%
  summarise(
    X2022_hogar = mean(X2022_hogar, na.rm = TRUE)
  ) %>%
  ungroup()

barrios_info <- barrios_info %>%
  left_join(renta_media_seleccion, by = c("coddistrit" = "Codigo_Distrito", 
                                          "codbarrio" = "Codigo_Barrio"))

```

Finalmente, se cambian los nombres del fichero por otros más orientativos:

```{r renombrarVar}
barrios_info <- barrios_info %>%
  rename(
    distrito = neighbourhood_group_cleansed,
    barrio = neighbourhood_cleansed,
    precio = media,
    coddistrito = coddistrit,
    renta_hogar = X2022_hogar
  )

head(barrios_info)
```

Ahora, una vez se tiene el fichero limpio y con las variables que se desean, se puede continuar con el estudio de agrupamiento de barrios.

# Influencia de las nuevas variables en el precio ponderado 

Para comenzar, se escalan los datos para que se puedan interpretar.

```{r escalar}
# Escalar las variables numéricas
barrios_scaled <- scale(barrios_info[, !(names(barrios_info) %in% c("distrito", "barrio", "codbarrio", "coddistrito"))])
barrios_scaled <- as.data.frame(barrios_scaled)

```

Para determinar qué variables influían en el precio, se optó por realizar una regresión lineal
```{r modelo lineal}
modelo <- lm(precio ~ distancia_P_Ayuntamiento + distancia_CAC + distancia_Bioparc + distancia_Estacion + distancia_Malvarrosa + distancia_Torres + distancia_UV + distancia_Puerto + distancia_Albufera + distancia_Viveros + distancia_UPV + distancia_Mestalla + emt + metrovlc + renta_hogar , data = barrios_scaled)
summary(modelo)
```
El modelo no sale como se esperaba. Por ejemplo, la distancia a la Plaza Ayuntamiento sería importante para los turistas y aquí no la es. Además, el error residual es muy alto (0.84) y la r^2 es algo baja (0.45). Se va a investigar por qué los datos no ajustan bien a la regresión, empezando por ver las distribuciones de las variables. 

```{r distribuciones var}

barrios_scaled_dist <- barrios_scaled %>%
  select(precio, distancia_P_Ayuntamiento, distancia_CAC, distancia_Bioparc, distancia_Estacion,
         distancia_Malvarrosa, distancia_Torres, distancia_UV, distancia_Puerto, distancia_Albufera,
         distancia_Viveros, distancia_UPV, distancia_Mestalla, emt, metrovlc,
         renta_hogar) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

# Plot histograms
ggplot(barrios_scaled_dist, aes(x = value)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  facet_wrap(~ variable, scales = "free") +
  theme_minimal() +
  labs(title = "Distributions of Variables", x = "", y = "Count")

```

Como muchas variables no siguen una distribución normal, se transforman los datos usando el logaritmo para ver si se pueden ajustar más a una distribución normal.

```{r transformar log,  warning=FALSE}
barrios_log <- barrios_scaled %>%
  filter(across(
    c(precio, distancia_P_Ayuntamiento, distancia_CAC, distancia_Bioparc, distancia_Estacion,
      distancia_Malvarrosa, distancia_Torres, distancia_UV, distancia_Puerto, distancia_Albufera,
      distancia_Viveros, distancia_UPV, distancia_Mestalla, emt, metrovlc,
      renta_hogar),
    ~ . > -1
  )) %>%
  mutate(across(
    everything(),
    ~ log(. + 1)
  ))
```

```{r modelo log}
modelo_log <- lm(precio ~ distancia_P_Ayuntamiento + distancia_CAC + distancia_Bioparc + distancia_Estacion + distancia_Malvarrosa + distancia_Torres + distancia_UV + distancia_Puerto + distancia_Albufera + distancia_Viveros + distancia_UPV + distancia_Mestalla + emt + metrovlc + renta_hogar , data = barrios_log)
summary(modelo_log)
```

```{r distribuciones log}
barrios_scaled_long_log <- barrios_log %>%
  select(precio, distancia_P_Ayuntamiento, distancia_CAC, distancia_Bioparc, distancia_Estacion,
         distancia_Malvarrosa, distancia_Torres, distancia_UV, distancia_Puerto, distancia_Albufera,
         distancia_Viveros, distancia_UPV, distancia_Mestalla, emt, metrovlc,
         renta_hogar) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

# Plot histograms
ggplot(barrios_scaled_long_log, aes(x = value)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  facet_wrap(~ variable, scales = "free") +
  theme_minimal() +
  labs(title = "Distributions of Variables", x = "", y = "Count")
```

Se observa que el modelo es mucho peor que el anterior, solo hay una única variable significativa, aunque el error haya bajado y la r^2 sea mayor. Viendo también las distribuciones, se nota que tampoco siguen una distribución normal. 

Se va a ver si los residuos son normales. 

```{r residuos}
plot(modelo$fitted.values, resid(modelo))
```

Como tiene una forma de nube, los residuos siguen una distribución normal y el problema no es este.
A continuación se comprueba si hay multicolinealidad o no mediante una matriz de correlaciones.

```{r correlaciones}
cor_matrix <- cor(barrios_scaled)
cor_matrix
```

El problema es este. Hay muchas variables muy correlacionadas entre sí que está afectando el modelo. Se va a ver el factor de inflación de la varianza (VIF). Si el VIF para las variables es mayor a 10, significa que el problema es grave. 

```{r vif}
vif(modelo)
```

# PCA

Se decidió hacer un PCA para reducir la dimensionalidad.
```{r pca}
barrios_scaled_sinprecio <- subset(barrios_scaled, select = -c(precio))
res.pca = PCA(barrios_scaled_sinprecio, scale.unit = FALSE, graph = FALSE, ncp = 10)
eig.val <- get_eigenvalue(res.pca)
VPmedio = 100 * (1/nrow(eig.val))
fviz_eig(res.pca, addlabels = TRUE) +
  geom_hline(yintercept=VPmedio, linetype=2, color="red")
kable(eig.val[1:6,])
K = 3
res.pca = PCA(barrios_scaled_sinprecio, scale.unit = FALSE, graph = FALSE, ncp = K)
```
Se observa que es la primera componente principal la que explica más de la mitad de la varianza.Se optó por elegir 3 componentes principales, puesto que son las que superan la línea roja.

Seguidamente se tratarán los aípicos mediante el T2 de Hotteling.

```{r pca atipicos}

# Gráfico T2 Hotelling
misScores = res.pca$ind$coord[,1:K]
miT2 = colSums(t(misScores**2)/eig.val[1:K,1])
I = nrow(barrios_scaled)
F95 = K*(I**2 - 1)/(I*(I - K)) * qf(0.95, K, I-K)
F99 = K*(I**2 - 1)/(I*(I - K)) * qf(0.99, K, I-K)

plot(1:length(miT2), miT2, type = "p", xlab = "Barrios", ylab = "T2")
abline(h = F95, col = "orange", lty = 2, lwd = 2)
abline(h = F99, col = "red3", lty = 2, lwd = 2)
anomalas = which(miT2 > F95)
anomalas
```
Si es verdad que hay algunos valores que se encuentran por encima de la línea roja y que se considerarían atípicos. Se dejarán en el estudio y se proseguirá con el análisis.

Se realiza un gráfico de individuos, coloreados por si son o no valores atípicos según el T2 de Hotelling.

```{r individuos}
p1 = fviz_pca_ind(res.pca, axes = c(1,2), geom = c("point"),
                  habillage = factor(miT2 > F95)) +
  tune::coord_obs_pred()

p2 = fviz_pca_ind(res.pca, axes = c(1,3), geom = c("point"), 
                  habillage = factor(miT2 > F95)) +
  tune::coord_obs_pred() 
  

grid.arrange(p1,p2, nrow = 1)
```
Los resultados para la dimensión 1 con la 2 y la 1 con la 3 son bastantes similares. Se observa que los puntos que están en rojo (los no atípicos) se encuentran más unidos y cercanos, mientras que los azules están más alejados (tiene sentido, ya que son valores más anómalos).

Se prosigue visualizando las variables.

```{r variables}
fviz_pca_var(res.pca, axes = c(1,2), repel = TRUE, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
fviz_pca_var(res.pca, axes = c(1,3), repel = TRUE, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
```
Se observa claramente que las variables que contribuyen más a la dimensión 1 son las variables de distancia, mientras que las que contribuyen más a la dimensión 3 son las de transporte (metrovlc y emt).

Para verlo de manera más clara, se va a visualizar las contribuciones de las variables para cada componente principal.

```{r contribucion pca1}
fviz_contrib(res.pca, choice = "var", axes = 1)
```
Componente 1: Distancias

```{r contribucion pca2}
fviz_contrib(res.pca, choice = "var", axes = 2)
```
Componente 2: Distancias

```{r contribucion pca3}
fviz_contrib(res.pca, choice = "var", axes = 3)
```
Componente 3: Transporte


Realmente lo que se quiere es predecir el valor de una variable (el precio) a partir de otras. Además, como hay multicolinealidad, es mejor hacer un análisis PLS, puesto que el PCA no usa la variable dependiente (precio).

Usamos PLS porque queremos predecir una variable dependiente a partir de varias independientes que presentan multicolinealidad y porque reduce la dimensionalidad y extrae componentes latentes que explican la mayor covarianza entre X e Y, mejorando la estabilidad y precisión del modelo.

# PLS

```{r pls}

X <- barrios_scaled %>%
  select(-precio)  # remove the target from predictors

y <- barrios_scaled$precio

pls_model <- plsr(y ~ ., data = X, scale = FALSE, validation = "CV")
summary(pls_model)
```
El modelo PLS logra capturar el 100% de la varianza de las variables independientes (X) con 15 componentes, pero solo explica un máximo del 45.3% de la varianza de la variable dependiente (Y). La validación cruzada indica que el error de predicción (RMSEP) es más bajo con 2 o 3 componentes, y luego comienza a aumentar, lo que sugiere sobreajuste al incluir demasiados componentes. Por lo tanto, un modelo con 2 o 3 componentes es el más adecuado, ya que ofrece el mejor equilibrio entre precisión y simplicidad.

Esto también se puede verificar en el siguiente gráfico:

```{r validacionpls}
validationplot(pls_model, val.type = "MSEP")
```

El error empieza bajo, luego aumenta y disminuye de nuevo: con 2 o 3 componentes el error de predicción es bastante bajo.

```{r pls loadings}
loadings <- pls_model$loading.weights[, 1:3]  
print(loadings)
```
Primera componente: Distancias
Segunda componente:renta y transporte

Seguidamente se calcula el VIP para 3 componentes, que es para seleccionar las variables mas relevantes para el modelo.

```{r vip componente 1}
VIP_single_component <- function(pls_model, component_number) {
  W <- pls_model$loading.weights
  Yload <- pls_model$Yloadings
  SS <- drop(Yload^2)[component_number]   # sum of squares for selected component
  Wnorm2 <- sum(W[, component_number]^2)
  
  vip_scores <- sqrt(ncol(W) * (W[, component_number]^2 * (SS / Wnorm2)) / SS)
  names(vip_scores) <- rownames(W)
  return(vip_scores)
}

vip_comp1 <- VIP_single_component(pls_model, 1)
sort(vip_comp1, decreasing = TRUE)
```

```{r vip componente 2}
vip_comp2 <- VIP_single_component(pls_model, 2)
sort(vip_comp2, decreasing = TRUE)
```

```{r vip componente 3}
vip_comp3 <- VIP_single_component(pls_model, 3)
sort(vip_comp3, decreasing = TRUE)
```

```{r ordenar vip}
vip_df <- data.frame(
  #variable = names(vip1),
  comp1 = vip_comp1,
  comp2 = vip_comp2,
  comp3 = vip_comp3
)
vip_df
vip_df %>%
  filter(comp1 > 1 | comp2 > 1 | comp3 > 1) %>%
  arrange(desc(comp1 + comp2 + comp3))
```
En la primera componente, las variables más importantes vuelven a ser las de las distancias. Por otra parte, en la segunda componente, son la renta_media y las variables de transporte las que toman protagonismo. En cuanto a la tercera componente, solo la emt y ciertas distancias son las importantes.

Finalmente se escogen dos únicas componentes. Se puede ver en la primera componente que las distancias no significativas segun el VIF son distancia_Puerto, distancia_Estacion, distancia_CAC, distancia_Bioparc (variables que no se tienen en cuenta a la hora del clustering). Además, tampoco son significativas las variables de transporte y renta, pero sí que lo son en la segunda componente (por tanto, estas variables no se quitan). 

# Agrupaciones de barrios (clusterización)

Por último, se va a realizar un clustering para agrupar las viviendas por barrios, teniendo en cuenta las nuevas características añadidas.

```{r clusteringdf}
col <- setdiff(names(barrios_info), c("distrito","barrio"))

# Se escalan todas las variables nuéricas
barrios_info[col]<- scale(barrios_info[col])
barrios_info
```
Para poder realizar los clusters, se pondera cada variable según su importancia en el modelo PLS, es decir, se multiplica cada variable por su coeficiente.

Además, se suman estas variables ponderadas para obtener un precio estimado. Finalmente, se combina este valor estimado con el precio promedio del barrio, generando un índice más completo.

```{r precio_pond_barrio}

coef <- coef(pls_model, ncomp=2) # Extrae los coeficientes del modelo PLS para los primeros 2 componentes.

# Se eliminan las variables que no son significativas según el PLS
barrios_significativas <- subset(barrios_scaled, select = -c(distancia_Puerto, distancia_Estacion, distancia_CAC, distancia_Bioparc, precio)) 

ponderado_barrio <- sweep(barrios_significativas, 2, coef, `*`)
ponderado_barrio$precio_barrio <- rowSums(ponderado_barrio)

barrios_info$precio_barrio <- (ponderado_barrio$precio_barrio + barrios_info$precio)
barrios_info
```
Comenzando con el clustering, se usa la distancia euclídea porque no se busca agrupar viviendas por perfil urbano o ubicación, sino por similitud en las variables cuantitativas (como precio, tamaño, distancia a puntos clave, etc.). La distancia euclídea mide justamente la cercanía numérica entre observaciones, sin importar su contexto geográfico.

```{r matriz distancias}
precio_barrio_df <- as.data.frame(barrios_info$precio_barrio)
midist <- get_dist(precio_barrio_df, stand = FALSE, method = "euclidean")
fviz_dist(midist, show_labels = TRUE, lab_size = 0.3,
          gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```

# Método de Ward

El método de Ward es un método jerárquico que busca minimizar la variabilidad interna dentro de cada grupo.
En nuestro caso, se combinará el análisis del coeficiente de Silhouette con la variabilidad intra-cluster. El coeficiente de Silhouette mide qué tan bien separado está cada punto dentro de su cluster comparado con otros clusters.

```{r Ward, warning=FALSE}
p1 = fviz_nbclust(x = precio_barrio_df, FUNcluster = hcut, method = "silhouette", 
                  hc_method = "ward.D2", k.max = 10, verbose = FALSE, 
                  hc_metric = "euclidean") + labs(title = "Num. optimo clusters")
p2 = fviz_nbclust(x = precio_barrio_df, FUNcluster = hcut, method = "wss", 
                  hc_method = "ward.D2", k.max = 10, verbose = FALSE, 
                  hc_metric = "euclidean") + labs(title = "Num. optimo clusters")
grid.arrange(p1, p2, nrow = 1)
```
Aunque en el gráfico de la izquierda el coeficiente de silhouette indique que el número de clusters óptimo es 2, si se observa el gráfico de la derecha se ve que para este número la suma de cuadrados aún es demasiado alta. Por tanto, se escogerán 4 clusters, ya que son los óptimos por el método del codo.

Los grupos son:
```{r clust1}
clust1 <- hclust(midist, method="ward.D2")
grupos1 <- cutree(clust1, k=4)
table(grupos1)
```

# Método de la media

El método de la media en un clustering jerárquico define la distancia entre dos clústeres como la media de las distancias entre todas las parejas de elementos que los componen.

```{r media, warning=FALSE}
p1 = fviz_nbclust(x = precio_barrio_df, FUNcluster = hcut, method = "silhouette", 
                  hc_method = "average", k.max = 10, verbose = FALSE, 
                  hc_metric = "euclidean") + labs(title = "Num. optimo clusters")
p2 = fviz_nbclust(x = precio_barrio_df, FUNcluster = hcut, method = "wss", 
                  hc_method = "average", k.max = 10, verbose = FALSE, 
                  hc_metric = "euclidean") + labs(title = "Num. optimo clusters")
grid.arrange(p1, p2, nrow = 1)
```
Para este método se han escogido 5 clusters como número óptimo, pues es el segundo coeficiente más alto de silhouette y la variabilidad intra cluster es bastante más baja que con 2.

Los grupos son:
```{r clust2}
clust2 <- hclust(midist, method="average")
grupos2 = cutree(clust2, k = 5)
table(grupos2)
```

## K-medias

El algoritmo k-means es un método no jerárquico de agrupamiento que busca dividir un conjunto de datos en k grupos o clusters minimizando la variación dentro de cada grupo.

```{r Kmedias}
p1 = fviz_nbclust(x = precio_barrio_df, FUNcluster = kmeans, method = "silhouette", 
             k.max = 10, verbose = FALSE) +
  labs(title = "K-means")
p2 = fviz_nbclust(x = precio_barrio_df, FUNcluster = kmeans, method = "wss", 
             k.max = 10, verbose = FALSE) +
  labs(title = "K-means")
grid.arrange(p1, p2, nrow = 1)
```
En el gráfico de la izquierda se ven coeficientes de silhouette similares para valores a partir del 2. Se escogen 4 clusters porque la suma de cuadrados ya es lo suficientemente baja.

Los grupos son los siguentes:
```{r clust3}
set.seed(100)
clust3 <- kmeans(precio_barrio_df, centers = 4, nstart = 20)
table(clust3$cluster)
```
## K-medoides

El algoritmo k-medoides es otro método no jerárquico que agrupa datos seleccionando como centro de cada cluster un medoide, que es un punto real cuyo total de distancias a los demás puntos del cluster es mínimo.

```{r Kmedoides}
p1 = fviz_nbclust(x = precio_barrio_df, FUNcluster = pam, method = "silhouette", 
             k.max = 10, verbose = FALSE) +
  labs(title = "Numero optimo de clusters")
p2 = fviz_nbclust(x = precio_barrio_df, FUNcluster = pam, method = "wss", 
             k.max = 10, verbose = FALSE) +
  labs(title = "Numero optimo de clusters")
grid.arrange(p1, p2, nrow = 1)
```
Para k-medoides el número de clusters óptimo también es 4, porque es donde se produce el codo en el gráfico de la derecha y el coeficielte de silhouette se mantiene alto.

Los grupos son los siguientes:
```{r clust4}
clust4 <- pam(precio_barrio_df, k = 4)
table(clust4$clustering)
```
# Validación del clustering

Para elegir el método de clustering más adecuado, es fundamental evaluar la calidad de los grupos generados y validar los resultados obtenidos. Por ello, se comparan las técnicas Ward, k-medias y k-medoides, utilizando el índice de silhouette como criterio de validación. El índice de silhouette mide qué tan bien se ajusta cada punto a su cluster asignado en comparación con otros clusters, proporcionando una métrica clara para evaluar la cohesión y separación de los grupos.
1- mejor asignación
```{r validacion}
colores = pal_npg("nrc")(6)
colores2 = pal_npg("nrc")(7)
par(mfrow = c(1,3))
plot(silhouette(grupos1, midist), col=colores, border=NA, main = "WARD")
plot(silhouette(clust3$cluster, midist), col=colores, border=NA, main = "K-MEDIAS")
plot(silhouette(clust4$clustering, midist), col=colores2, border=NA, main = "K-MEDOIDES")
```
El análisis muestra diferencias pero no tan claras en la calidad de los clusters obtenidos. Valores cercanos a 1 del coeficiente de silhouette indican buena cohesión y separación.

En ningún método el coeficiente de clustering es negativo, por lo que los tres modelos hacen bien la agrupación de clusters.

Para salir de dudas, se probará la validación con la función clValid, probando tres métodos diferentes: jerárquico (ward), k-medias y PAM (k-medoides). Se evaluaron agrupaciones con 4 y 5 clusters utilizando la distancia euclídea y validación interna para comparar la calidad de los clusters. Finalmente, se examinó un resumen de los resultados para determinar la mejor opción.


```{r val}
metodos = c("ward","kmeans","pam")
validacion = suppressMessages(clValid(precio_barrio_df, nClust = 4:5, metric = "euclidean", 
                      clMethods = metodos, 
                      validation = c("internal"),
                      method = "ward"))
summary(validacion)
```
En resumen, K-Medias ofrece la mejor estructura de clusters para este conjunto de datos, con mayor cohesión y separación, mientras que Ward y K-Medoides presentan agrupamientos menos claros y posibles problemas en la asignación de puntos. Por tanto, el número de clusters óptimo escogido es 4.

# Separación por cluster

Para facilitar el analisis de los objetivos 3 y 4 siguientes, se separarán los clusters y se creará un fichero .csv con cada uno de ellos.

```{r clustcsv}
barrios_info$cluster <- clust3$cluster
cluster1 <- subset(barrios_info, cluster == 1)
cluster2 <- subset(barrios_info, cluster == 2)
cluster3 <- subset(barrios_info, cluster == 3)
cluster4 <- subset(barrios_info, cluster == 4)


write.csv(cluster1,"cluster1.csv")
write.csv(cluster2,"cluster2.csv")
write.csv(cluster3,"cluster3.csv")
write.csv(cluster4,"cluster4.csv")

```

Por último, a modo de visualizar como se distribuyen estos clusters en valencia, es decir, como se agrupan los distintos barrios de la ciudad, se ha realizado un mapa interactivo, donde se le asocia un color a cada cluster.
s
```{r mapa clust}

# Unir los clusters 
valencia_geo <- left_join(barrios, barrios_info, by = c("nombre" = "barrio"))

# Asegura de que cluster es un factor o numérico
valencia_geo$cluster <- as.factor(valencia_geo$cluster)

# Definir colores para 4 clusters
pal <- colorFactor(c("red", "blue", "green", "orange"), domain = levels(valencia_geo$cluster))

# Crear el mapa
leaflet(data = valencia_geo) %>%
  addTiles() %>%
  addPolygons(
    fillColor = ~pal(cluster),
    weight = 1,
    opacity = 1,
    color = "white",
    dashArray = "3",
    fillOpacity = 0.7,
    popup = ~paste("Barrio:", nombre, "<br>Cluster:", cluster)
  ) %>%
  addLegend(
    position = "bottomright",
    pal = pal,
    values = ~cluster,
    title = "Clusters",
    opacity = 1
  )

```